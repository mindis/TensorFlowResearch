{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaoran/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "# Some personnal imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmnist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the logistic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta_regul * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 19.309465\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 12.7%\n",
      "Minibatch loss at step 500: 2.463279\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 1000: 1.775184\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 1500: 0.983707\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 2000: 0.856673\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 2500: 0.862013\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 3000: 0.778380\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.9%\n",
      "Test accuracy: 88.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L2 regularization introduces a new meta parameter that should be tuned. Since I do not have any idea of what should be the right value for this meta parameter, I will plot the accuracy by the meta parameter value (in a logarithmic scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhgAAAF4CAYAAAAWmIDXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4AeKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a52e710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the same technique will improve the prediction of the 1-layer neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 590.374023\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 25.4%\n",
      "Minibatch loss at step 500: 199.734955\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 1000: 116.521393\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 1500: 68.802231\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 2000: 41.379978\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 2500: 25.250950\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 3000: 15.515349\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.5%\n",
      "Test accuracy: 93.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally something above 90%! I will also plot the final accuracy by the L2 parameter to find the best value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:    \n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgoAAAF4CAYAAAA1w9ECAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x147bbefd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (1-layer net)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 374.823853\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 29.1%\n",
      "Minibatch loss at step 2: 1806.397705\n",
      "Minibatch accuracy: 32.8%\n",
      "Validation accuracy: 29.3%\n",
      "Minibatch loss at step 4: 344.058014\n",
      "Minibatch accuracy: 47.7%\n",
      "Validation accuracy: 50.8%\n",
      "Minibatch loss at step 6: 17.413076\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 59.6%\n",
      "Minibatch loss at step 8: 0.316470\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 60.7%\n",
      "Minibatch loss at step 10: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 12: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 14: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 16: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 18: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 20: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 22: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 24: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 26: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 28: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 32: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 34: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 36: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 38: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 40: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 42: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 44: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 46: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 48: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 50: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 52: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 54: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 56: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 58: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 62: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 64: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 66: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 68: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 76: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 78: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 82: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 84: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 86: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 88: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 92: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 94: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 96: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 60.0%\n",
      "Test accuracy: 65.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_bacthes = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_bacthes\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are far too much parameters and no regularization, the accuracy of the batches is 100%. The generalization capability is poor, as shown in the validation and test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  logits = tf.matmul(drop1, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 522.686890\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 29.1%\n",
      "Minibatch loss at step 2: 814.265259\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 28.4%\n",
      "Minibatch loss at step 4: 300.448486\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 54.8%\n",
      "Minibatch loss at step 6: 24.725126\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 8: 2.053320\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 10: 26.185101\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 62.2%\n",
      "Minibatch loss at step 12: 74.086914\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 66.1%\n",
      "Minibatch loss at step 14: 16.961481\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 67.2%\n",
      "Minibatch loss at step 16: 0.000043\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.5%\n",
      "Minibatch loss at step 18: 1.931412\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.1%\n",
      "Minibatch loss at step 20: 3.458273\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 22: 0.269873\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 24: 6.727062\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 67.4%\n",
      "Minibatch loss at step 26: 1.342917\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 28: 3.533568\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.1%\n",
      "Minibatch loss at step 30: 2.286844\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 32: 0.303651\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 34: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 36: 0.939636\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.4%\n",
      "Minibatch loss at step 38: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.4%\n",
      "Minibatch loss at step 40: 0.764459\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 65.7%\n",
      "Minibatch loss at step 42: 0.769536\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 44: 2.977767\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 46: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.5%\n",
      "Minibatch loss at step 48: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.6%\n",
      "Minibatch loss at step 50: 3.058991\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.9%\n",
      "Minibatch loss at step 52: 0.909829\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.7%\n",
      "Minibatch loss at step 54: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.7%\n",
      "Minibatch loss at step 56: 1.273322\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 58: 2.320458\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.5%\n",
      "Minibatch loss at step 60: 0.501477\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 62: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.7%\n",
      "Minibatch loss at step 64: 0.263712\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.9%\n",
      "Minibatch loss at step 66: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.9%\n",
      "Minibatch loss at step 68: 1.202995\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 70: 1.206359\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 76: 1.197501\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 78: 1.862640\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 80: 0.206966\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.4%\n",
      "Minibatch loss at step 82: 0.051613\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.3%\n",
      "Minibatch loss at step 84: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.4%\n",
      "Minibatch loss at step 86: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 88: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 92: 0.589971\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 94: 1.581323\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 96: 2.395708\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.7%\n",
      "Test accuracy: 73.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first conclusion is that 100% of accuracy on the minibatches is more difficult achieved or to keep. As a result, the test accuracy is improved by 6%, the final net is more capable of generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a first try with 2 layers. Note how the parameters are initialized, compared to the previous cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 100\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.272147\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 34.4%\n",
      "Minibatch loss at step 500: 0.930104\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 1000: 0.904542\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 1500: 0.575127\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 2000: 0.520965\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 2500: 0.531228\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 3000: 0.565390\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 3500: 0.573201\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 4000: 0.445847\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 4500: 0.444020\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 5000: 0.498980\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 5500: 0.493428\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 6000: 0.563357\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 6500: 0.390322\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 7000: 0.506404\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 7500: 0.472213\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 8000: 0.571431\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 8500: 0.409382\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 9000: 0.470708\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.0%\n",
      "Test accuracy: 95.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is getting really good. Let's try one layer deeper with dropouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "  logits = tf.matmul(lay3_train, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.322881\n",
      "Minibatch accuracy: 16.4%\n",
      "Validation accuracy: 42.7%\n",
      "Minibatch loss at step 500: 0.366416\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 1000: 0.338523\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 1500: 0.455589\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 2000: 0.257622\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 2500: 0.386977\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 3000: 0.300966\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 3500: 0.403921\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 4000: 0.311486\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 4500: 0.192245\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 5000: 0.206171\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 5500: 0.297852\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 6000: 0.196558\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 6500: 0.103116\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 7000: 0.232592\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 7500: 0.164838\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 8000: 0.174380\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 8500: 0.138230\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 9000: 0.149158\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 9500: 0.116271\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 10000: 0.166820\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 10500: 0.187102\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 11000: 0.130559\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 11500: 0.071675\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 12000: 0.072376\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 12500: 0.147814\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 13000: 0.117502\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 13500: 0.096444\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 14000: 0.085476\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 14500: 0.108866\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 15000: 0.148186\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 15500: 0.065941\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 16000: 0.077021\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 16500: 0.079135\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 17000: 0.129873\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 17500: 0.128750\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 18000: 0.052478\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.4%\n",
      "Test accuracy: 96.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 18001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huge! That's my best score on this dataset. I have also tried more parameters, but it does not help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 512\n",
    "num_hidden_nodes3 = 256\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(drop1, weights2) + biases2)\n",
    "  drop2 = tf.nn.dropout(lay2_train, 0.5)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(drop2, weights3) + biases3)\n",
    "  drop3 = tf.nn.dropout(lay3_train, 0.5)\n",
    "  logits = tf.matmul(drop3, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 5000, 0.80, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.830509\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 15.0%\n",
      "Minibatch loss at step 500: 0.605592\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 1000: 0.522174\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 1500: 0.567506\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 2000: 0.378444\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 2500: 0.485396\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 3000: 0.497009\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 3500: 0.546354\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 4000: 0.583119\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 4500: 0.385595\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 5000: 0.393430\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 5500: 0.613778\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 6000: 0.352996\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 6500: 0.388617\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 7000: 0.402814\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 7500: 0.450085\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 8000: 0.466122\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 8500: 0.272252\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 9000: 0.472153\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 9500: 0.472621\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 10000: 0.429850\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.5%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9de5e9efb445>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     _, l, predictions = session.run(\n\u001b[0;32m---> 18\u001b[0;31m       [optimizer, loss, train_prediction], feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Minibatch loss at step %d: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chaoran/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 372\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    373\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chaoran/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 636\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    637\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m       \u001b[0;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chaoran/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 708\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    709\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/chaoran/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    713\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chaoran/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    695\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    696\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
